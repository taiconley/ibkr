{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/ibkr\")\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import pytz\n",
    "\n",
    "\n",
    "import passwords\n",
    "from databaseClass import DB\n",
    "from utils import generate_df_from_sql_file, generate_list_from_sql_file\n",
    "from utils import DataProcessor\n",
    "from utils import ModelBuilder\n",
    "from utils import Predictor\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578ac9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3385596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "userName = passwords.userName\n",
    "userPass = passwords.userPass\n",
    "dataBaseName = passwords.dataBaseName\n",
    "host = passwords.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DB(userName=userName, userPass=userPass, dataBaseName=dataBaseName, host='ibkr_db', docker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99521d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db.tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40de832",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sql_file='../sql_files/test.sql'\n",
    "df = generate_df_from_sql_file(input_sql_file, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d715640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    # Set 'timestamp' as the index\n",
    "    df = df.set_index('timestamp')\n",
    "    \n",
    "    # Pivot the table and also include 'volume' where ticktype is 5\n",
    "    df_pivot = df.pivot_table(index=df.index, columns='ticktype', values=['price', 'volume'])\n",
    "    df_pivot.columns = ['_'.join(map(str,i)) for i in df_pivot.columns]\n",
    "    \n",
    "    # Resample the data per second and fill forward any NaN values\n",
    "    df_resampled = df_pivot.resample('1S').agg({'price_1': 'last', 'price_2': 'last', 'price_4': 'last', 'volume_5': 'sum'}).ffill()\n",
    "    \n",
    "    #reshape\n",
    "    df_resampled['Open'] = df_resampled['price_1']\n",
    "    df_resampled['High'] = df_resampled[['price_1', 'price_2', 'price_4']].max(axis=1)\n",
    "    df_resampled['Low'] = df_resampled[['price_1', 'price_2', 'price_4']].min(axis=1)\n",
    "    df_resampled['Close'] = df_resampled['price_4']\n",
    "    df_resampled['Volume'] = df_resampled['volume_5']\n",
    "    df_resampled = df_resampled[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "start_time = time.time()\n",
    "processed_df = process_df(df)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution time:\", execution_time, \"seconds\")\n",
    "\n",
    "def scale_shift_data(df, look_ahead):\n",
    "    # Normalize the dataset\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(df)\n",
    "\n",
    "    # Convert scaled array into dataframe\n",
    "    df_scaled = pd.DataFrame(scaled, index=df.index, columns=df.columns)\n",
    "\n",
    "    # Shift the dataframe to create the labels\n",
    "    df_scaled_shifted = df_scaled.shift(-look_ahead)\n",
    "\n",
    "    # Drop the last 'look_ahead' rows\n",
    "    df_scaled = df_scaled.iloc[:-look_ahead]\n",
    "    df_scaled_shifted = df_scaled_shifted.iloc[:-look_ahead]\n",
    "\n",
    "    return df_scaled, df_scaled_shifted['Close'], scaler\n",
    "\n",
    "# Preparing data for 5 second prediction\n",
    "X, y, scaler_5s = scale_shift_data(processed_df, look_ahead=5)\n",
    "\n",
    "# Preparing data for 1 minute prediction\n",
    "X, y, scaler_1m = scale_shift_data(processed_df, look_ahead=60)\n",
    "\n",
    "def create_dataset(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 60\n",
    "\n",
    "# Reshape to [samples, time_steps, n_features]\n",
    "X_5s, y_5s = create_dataset(X, y, TIME_STEPS)\n",
    "X_1m, y_1m = create_dataset(X, y, TIME_STEPS)\n",
    "\n",
    "train_size = int(len(X_5s) * 0.8)\n",
    "test_size = len(X_5s) - train_size\n",
    "\n",
    "X_train_5s, X_test_5s = X_5s[0:train_size], X_5s[train_size:len(X_5s)]\n",
    "y_train_5s, y_test_5s = y_5s[0:train_size], y_5s[train_size:len(y_5s)]\n",
    "\n",
    "X_train_1m, X_test_1m = X_1m[0:train_size], X_1m[train_size:len(X_1m)]\n",
    "y_train_1m, y_test_1m = y_1m[0:train_size], y_1m[train_size:len(y_1m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "\n",
    "# Number of features in the data. In your case, it's 4 (price_1, price_2, price_4, volume_5)\n",
    "n_features = 5 \n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(TIME_STEPS, n_features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_5s, y_train_5s, epochs=20, batch_size=64, validation_data=(X_test_5s, y_test_5s), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01424581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can plot the loss with:\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b11ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a compiled model identical to the previous one\n",
    "model = load_model('../models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_build_model(df, look_ahead=5):\n",
    "    \n",
    "    df['timestamp'] = df['timestamp'].dt.tz_localize('UTC') #adding this to update to utc\n",
    "\n",
    "    # Create a DataProcessor instance\n",
    "    processor = DataProcessor(df) \n",
    "    # Process the df\n",
    "    processor.process_df()\n",
    "    \n",
    "    # Scale and shift the data\n",
    "    scaler, close_scaler = processor.scale_shift_data(look_ahead)\n",
    "    # Create the X and y datasets\n",
    "    X, y = processor.scaled_df, processor.shifted_df['Close']\n",
    "    # Create train test split\n",
    "    X_train, X_test, y_train, y_test = processor.create_train_test_split(X, y)\n",
    "    # Create final train and test datasets\n",
    "    TIME_STEPS = 60\n",
    "    X_train, y_train = processor.create_dataset(X_train, y_train, TIME_STEPS)\n",
    "    X_test, y_test = processor.create_dataset(X_test, y_test, TIME_STEPS)\n",
    "    # Number of features in the data\n",
    "    n_features = X_train.shape[2]\n",
    "    \n",
    "    # Create a ModelBuilder instance and build the model\n",
    "    builder = ModelBuilder(n_features, TIME_STEPS)\n",
    "    # Train the model\n",
    "    model, history = builder.train_model(X_train, y_train, X_test, y_test, epochs=20, batch_size=64)\n",
    "    # Save the model\n",
    "    model_path = '../models/model.h5'\n",
    "    builder.save_model(model_path)\n",
    "    # Plot loss\n",
    "    builder.plot_loss(history)\n",
    "    # Create a Predictor instance\n",
    "    predictor = Predictor(model, processor)\n",
    "    # Predict\n",
    "    predictions = predictor.predict(look_ahead, TIME_STEPS)\n",
    "    # Rescale predictions\n",
    "    rescaled_predictions = predictor.rescale_prediction(predictions)\n",
    "    return rescaled_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_build_model(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
